{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "from functools import partial\n",
    "from copy import deepcopy\n",
    "from survlimepy import SurvLimeExplainer\n",
    "from survlimepy.utils.neighbours_generator import NeighboursGenerator\n",
    "from survlimepy.load_datasets import RandomSurvivalData\n",
    "from sksurv.linear_model import CoxPHSurvivalAnalysis\n",
    "from sksurv.nonparametric import nelson_aalen_estimator\n",
    "import sklearn\n",
    "from sklearn.utils import check_random_state\n",
    "import numpy as np\n",
    "import cvxpy as cp\n",
    "from cvxpy.atoms.affine.binary_operators import MulExpression as MulExp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate data\n",
    "n_points = 500\n",
    "#true_coef = [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]\n",
    "true_coef = [1, 1]\n",
    "r = 1\n",
    "#center = [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
    "center = [0, 0]\n",
    "prob_event = 0.9\n",
    "lambda_weibull = 10**(-6)\n",
    "v_weibull = 2\n",
    "n_features = len(true_coef)\n",
    "\n",
    "rsd = RandomSurvivalData(\n",
    "    center=center,\n",
    "    radius=r,\n",
    "    coefficients=true_coef,\n",
    "    prob_event=prob_event,\n",
    "    lambda_weibull=lambda_weibull,\n",
    "    v_weibull=v_weibull,\n",
    "    time_cap=None\n",
    ")\n",
    "\n",
    "# Train\n",
    "X, time_to_event, delta = rsd.random_survival_data(num_points=n_points)\n",
    "z = [(d, t) for d, t in zip(delta, time_to_event)]\n",
    "y = np.array(z, dtype=[(\"delta\", np.bool_), (\"time_to_event\", np.float32)])\n",
    "total_row_train = X.shape[0]\n",
    "print('total_row_train:', total_row_train)\n",
    "unique_times = np.sort(np.unique(time_to_event))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fit a Cox model\n",
    "cox = CoxPHSurvivalAnalysis()\n",
    "cox.fit(X, y)\n",
    "print(cox.coef_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# SurvLime for COX\n",
    "num_samples = 1000\n",
    "max_difference_time_allowed = None\n",
    "max_hazard_value_allowed = None\n",
    "x_new = center\n",
    "explainer_cox = SurvLimeExplainer(\n",
    "    training_features=X,\n",
    "    training_events=[tp[0] for tp in y],\n",
    "    training_times=[tp[1] for tp in y],\n",
    "    model_output_times=cox.event_times_,\n",
    "    sample_around_instance=True,\n",
    "    random_state=10,\n",
    ")\n",
    "\n",
    "b_cox = explainer_cox.explain_instance(\n",
    "    data_row=x_new,\n",
    "    predict_fn=cox.predict_cumulative_hazard_function,\n",
    "    num_samples=num_samples,\n",
    "    max_difference_time_allowed=max_difference_time_allowed,\n",
    "    max_hazard_value_allowed=max_hazard_value_allowed,\n",
    "    verbose=False,\n",
    ")\n",
    "\n",
    "print(b_cox)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Kernel\n",
    "kernel_width = np.sqrt(len(x_new)) * 0.75\n",
    "def custom_kernel_width(d: np.ndarray, kernel_width: float) -> np.ndarray:\n",
    "    return np.sqrt(np.exp(-(d**2) / kernel_width**2))\n",
    "\n",
    "kernel_fn = partial(custom_kernel_width, kernel_width=kernel_width)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate the neighbours\n",
    "data_point = np.array(x_new).reshape(1, -1)\n",
    "neighbours_generator = NeighboursGenerator(\n",
    "    training_features=X,\n",
    "    data_row=data_point,\n",
    "    random_state=check_random_state(10),\n",
    ")\n",
    "scaled_data = neighbours_generator.generate_neighbours(num_samples=num_samples, sample_around_instance=True)\n",
    "print(scaled_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_nelson_aalen_estimator(event: np.ndarray, time: np.ndarray) -> np.ndarray:\n",
    "        nelson_aalen = nelson_aalen_estimator(event, time)\n",
    "        H0 = nelson_aalen[1]\n",
    "        m = H0.shape[0]\n",
    "        H0 = np.reshape(H0, newshape=(m, 1))\n",
    "        return H0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#b = cp.Variable((len(x_new), 1))\n",
    "#b = np.array(true_coef).reshape(-1, 1)\n",
    "b = np.array(\n",
    "    [\n",
    "        144.62512727, 461.45983917, 92.86307043,\n",
    "        -37667.68360483, 328.24800985, -124.94553269,\n",
    "        1119.01058628, -2037.22348856, -58136.93066531,\n",
    "        666.83663439, -316.71479306, -238.55402923,\n",
    "        299.33087086\n",
    "    ]\n",
    ").reshape(-1, 1)\n",
    "training_events=[tp[0] for tp in y]\n",
    "training_times=[tp[1] for tp in y]\n",
    "kernel_distance = \"euclidean\"\n",
    "epsilon = 10 ** (-6)\n",
    "num_features = len(x_new)\n",
    "unique_times_to_event = np.sort(unique_times)\n",
    "m = unique_times_to_event.shape[0]\n",
    "print(\"m: \", m)\n",
    "FN_pred = cox.predict_cumulative_hazard_function(scaled_data, return_array=True)\n",
    "print(\"FN_pred.shape:\", FN_pred.shape)\n",
    "print(\"max FN_pred:\", np.max(FN_pred))\n",
    "print(\"min FN_pred:\", np.min(FN_pred))\n",
    "H0 = compute_nelson_aalen_estimator(training_events, training_times)\n",
    "print(\"H0.shape:\", H0.shape)\n",
    "print(\"max H0:\", np.max(H0))\n",
    "print(\"min H0:\", np.min(H0))\n",
    "distances = sklearn.metrics.pairwise_distances(scaled_data, data_point, metric=kernel_distance).ravel()\n",
    "#print(\"distances:\", distances)\n",
    "H_score = deepcopy(FN_pred)\n",
    "H_score = np.clip(a=H_score, a_min=None, a_max=10)\n",
    "print(\"H_score.shape:\", H_score.shape)\n",
    "print(\"max H_score:\", np.max(H_score))\n",
    "print(\"min H_score:\", np.min(H_score))\n",
    "log_H = np.log(H_score + epsilon)\n",
    "print(\"log_H.shape:\", log_H.shape)\n",
    "print(\"max log_H:\", np.max(log_H))\n",
    "print(\"min log_H:\", np.min(log_H))\n",
    "log_correction = np.divide(H_score, log_H)\n",
    "print(\"log_correction.shape:\", log_correction.shape)\n",
    "print(\"max log_correction:\", np.max(log_correction))\n",
    "print(\"min log_correction:\", np.min(log_correction))\n",
    "H = np.reshape(np.array(H_score), newshape=(num_samples, m))\n",
    "print(\"H.shape:\", H.shape)\n",
    "print(\"max H:\", np.max(H))\n",
    "print(\"min H:\", np.min(H))\n",
    "LnH = np.log(H + epsilon)\n",
    "print(\"LnH.shape:\", LnH.shape)\n",
    "print(\"max LnH:\", np.max(LnH))\n",
    "print(\"min LnH:\", np.min(LnH))\n",
    "LnH0 = np.log(H0 + epsilon)\n",
    "print(\"LnH0.shape:\", LnH0.shape)\n",
    "print(\"max LnH0:\", np.max(LnH0))\n",
    "print(\"min LnH0:\", np.min(LnH0))\n",
    "logs = np.reshape(log_correction, newshape=(num_samples, m))\n",
    "print(\"logs.shape:\", logs.shape)\n",
    "print(\"max logs:\", np.max(logs))\n",
    "print(\"min logs:\", np.min(logs))\n",
    "weights = kernel_fn(distances)\n",
    "print(\"weights.shape:\", weights.shape)\n",
    "print(\"max weights:\", np.max(weights))\n",
    "print(\"min weights:\", np.min(weights))\n",
    "w = np.reshape(weights, newshape=(num_samples, 1))\n",
    "print(\"w.shape:\", w.shape)\n",
    "print(\"max w:\", np.max(w))\n",
    "print(\"min w:\", np.min(w))\n",
    "# Time differences\n",
    "t = np.empty(shape=(m + 1, 1))\n",
    "t[:m, 0] = unique_times_to_event\n",
    "t[m, 0] = t[m - 1, 0] + epsilon\n",
    "delta_t = [min(t[i + 1, 0] - t[i, 0], max_difference_time_allowed)  for i in range(m)]\n",
    "delta_t = np.reshape(np.array(delta_t), newshape=(m, 1))\n",
    "print(\"delta_t.shape:\", delta_t.shape)\n",
    "print(\"max delta_t:\", np.max(delta_t))\n",
    "print(\"min delta_t:\", np.min(delta_t))\n",
    "# Matrices to produce the proper sizes\n",
    "is_numpy = isinstance(b, np.ndarray)\n",
    "\n",
    "ones_N = np.ones(shape=(num_samples, 1))\n",
    "ones_m_1 = np.ones(shape=(m, 1))\n",
    "B = np.dot(ones_N, LnH0.T)\n",
    "C = LnH - B\n",
    "print(\"b.shape\", b.shape)\n",
    "Z =  scaled_data @ b\n",
    "if is_numpy:\n",
    "    print(\"Z.shape:\", Z.shape)\n",
    "    print(\"max Z:\", np.max(Z))\n",
    "    print(\"min Z:\", np.min(Z))\n",
    "D = Z @ ones_m_1.T\n",
    "E = C - D\n",
    "if is_numpy:\n",
    "    print(\"E.shape:\", E.shape)\n",
    "    print(\"max E:\", np.max(E))\n",
    "    print(\"min E:\", np.min(E))\n",
    "V_sq = np.square(log_correction)\n",
    "if is_numpy:\n",
    "    print(\"V_sq.shape:\", V_sq.shape)\n",
    "    print(\"max V_sq:\", np.max(V_sq))\n",
    "    print(\"min V_sq:\", np.min(V_sq))\n",
    "if is_numpy:\n",
    "    E_norm = np.power(E, 2)\n",
    "else:\n",
    "    E_norm = cp.power(E, 2)\n",
    "if is_numpy:\n",
    "    print(\"E_norm.shape:\", E_norm.shape)\n",
    "    print(\"max E_norm:\", np.max(E_norm))\n",
    "    print(\"min E_norm:\", np.min(E_norm))\n",
    "if is_numpy:\n",
    "    F = np.multiply(E_norm, V_sq)\n",
    "else:\n",
    "    F = cp.multiply(E_norm, V_sq)\n",
    "if is_numpy:\n",
    "    print(\"F.shape:\", F.shape)\n",
    "    print(\"max F:\", np.max(F))\n",
    "    print(\"min F:\", np.min(F))\n",
    "G = F @ delta_t\n",
    "if is_numpy:\n",
    "    print(\"G.shape:\", G.shape)\n",
    "    print(\"max G:\", np.max(G))\n",
    "    print(\"min G:\", np.min(G))\n",
    "funct = G.T @ weights\n",
    "if not is_numpy:\n",
    "    objective = cp.Minimize(funct)\n",
    "    prob = cp.Problem(objective)\n",
    "    result = prob.solve(verbose=True)\n",
    "    print(b.value)\n",
    "if is_numpy:\n",
    "    print(\"funct:\", funct)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scaled_data.shape"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv_survLIME",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "5125f1d6569b6fddfcceea96a1a5016994ac21200cfc124cf085e7070af32878"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
